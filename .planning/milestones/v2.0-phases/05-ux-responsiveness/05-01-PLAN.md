---
phase: 05-ux-responsiveness
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/providers/base.ts
  - src/providers/base-provider.ts
  - src/providers/anthropic.ts
  - src/providers/openai-compat.ts
  - src/ui/types.ts
  - src/ui/components.ts
  - src/ui/renderer.ts
  - src/cli/generate.ts
autonomous: true

must_haves:
  truths:
    - 'During an active LLM round, the terminal displays a live token counter that updates in place without scrolling'
    - 'During an active LLM round, the terminal displays a live elapsed timer that updates in place'
    - 'When a round completes, the progress line is replaced with a static summary showing final token count and duration'
    - 'Completed round summaries stack visibly as rounds finish'
    - 'The live token count during streaming is replaced by the authoritative API usage count on completion'
  artifacts:
    - path: 'src/providers/base.ts'
      provides: 'onToken callback in LLMProvider.complete() options'
      contains: 'onToken'
    - path: 'src/providers/base-provider.ts'
      provides: 'onToken threading from complete() to doComplete()'
      contains: 'onToken'
    - path: 'src/providers/anthropic.ts'
      provides: 'Streaming via messages.stream() with input_json_delta counting'
      contains: 'messages.stream'
    - path: 'src/providers/openai-compat.ts'
      provides: 'Streaming via chat.completions.stream() with argument delta counting'
      contains: 'completions.stream'
    - path: 'src/ui/types.ts'
      provides: 'streamingTokens and roundStartMs fields on RoundDisplayState'
      contains: 'streamingTokens'
    - path: 'src/ui/components.ts'
      provides: 'Live progress line format matching locked decision'
      contains: 'tokens.*total'
    - path: 'src/ui/renderer.ts'
      provides: 'Spinner tick updates elapsedMs for running rounds'
      contains: 'roundStartMs'
    - path: 'src/cli/generate.ts'
      provides: 'onToken callback wired from generate to providers via onStepStart'
      contains: 'onToken'
  key_links:
    - from: 'src/providers/anthropic.ts'
      to: 'src/ui/renderer.ts'
      via: 'onToken callback threaded through generate.ts orchestrator events'
      pattern: 'onToken.*streamingTokens'
    - from: 'src/ui/renderer.ts'
      to: 'src/ui/components.ts'
      via: 'spinner tick calls buildRoundLines which calls renderRoundBlock'
      pattern: 'buildRoundLines.*renderRoundBlock'
---

<objective>
Add streaming token callbacks to both LLM providers and wire a live progress display showing token count and elapsed timer during each active round.

Purpose: Transform the 30-90 second frozen wait into an interactive experience where users see tokens counting up and elapsed time ticking during each LLM round. This is the foundation for all Phase 5 UX improvements.

Output: Both providers stream tokens via `onToken` callback. TerminalRenderer shows live "Round N/6 ... X tokens (Y total) ... Zs" progress that updates in place. Completed rounds stack as static summaries.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-ux-responsiveness/05-RESEARCH.md

@src/providers/base.ts
@src/providers/base-provider.ts
@src/providers/anthropic.ts
@src/providers/openai-compat.ts
@src/ui/types.ts
@src/ui/components.ts
@src/ui/renderer.ts
@src/ui/formatters.ts
@src/cli/generate.ts
@src/domain/types.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add onToken streaming callback to provider layer</name>
  <files>
    src/providers/base.ts
    src/providers/base-provider.ts
    src/providers/anthropic.ts
    src/providers/openai-compat.ts
  </files>
  <action>
Thread an `onToken` callback through the provider stack so both LLM providers can stream token count updates during generation.

**src/providers/base.ts** — Extend the `complete()` options parameter:

- Add `onToken?: (tokenCount: number) => void` to the existing options object type in the `complete()` signature. The options parameter already has `onRetry`; add `onToken` alongside it.

**src/providers/base-provider.ts** — Thread `onToken` from `complete()` to `doComplete()`:

- Add `onToken?: (tokenCount: number) => void` as a third parameter to the abstract `doComplete()` method signature.
- In the `complete()` method, pass `options?.onToken` through to `doComplete()` inside the `retryWithBackoff` call. This means `retryWithBackoff(() => this.doComplete(request, schema, options?.onToken), ...)`.

**src/providers/anthropic.ts** — Implement streaming when `onToken` is provided:

- Add `onToken?: (tokenCount: number) => void` parameter to `doComplete()`.
- When `onToken` is provided, use `this.client.messages.stream({...})` instead of `this.client.messages.create({...})`. Pass the same parameters (model, max_tokens, system, messages, tools, tool_choice, temperature).
- Iterate the stream with `for await (const event of stream)`. For events where `event.type === 'content_block_delta' && event.delta.type === 'input_json_delta'`, accumulate character count from `event.delta.partial_json.length`. Estimate tokens as `Math.ceil(charCount / 4)` and call `onToken(estimatedTokens)`.
- After iteration, call `const message = await stream.finalMessage()` to get the complete response.
- Extract the `tool_use` block from `message.content` the same way the existing non-streaming path does.
- Get authoritative token count from `message.usage.output_tokens` and call `onToken(message.usage.output_tokens)` one final time to snap the counter to the real value.
- Parse with `schema.parse(toolBlock.input)` and return the same `CompletionResult & { data: T }` shape.
- Keep the existing non-streaming path as the `else` branch (when `onToken` is undefined). Do NOT delete the existing `messages.create()` code.

**src/providers/openai-compat.ts** — Implement streaming when `onToken` is provided:

- Add `onToken?: (tokenCount: number) => void` parameter to `doComplete()`.
- When `onToken` is provided, use `this.client.chat.completions.stream({...})` instead of `this.client.chat.completions.create({...})`. Pass the same parameters.
- Listen for chunks: `runner.on('chunk', (chunk) => { ... })`. Extract argument deltas from `chunk.choices[0]?.delta?.tool_calls?.[0]?.function?.arguments ?? ''`. Accumulate character count and estimate tokens as `Math.ceil(charCount / 4)`. Call `onToken(estimatedTokens)`.
- After the stream completes, call `const completion = await runner.finalChatCompletion()`.
- Extract `tool_calls[0].function.arguments`, parse with `JSON.parse(...)` then `schema.parse(...)`.
- Call `onToken(completion.usage?.completion_tokens ?? estimatedTokens)` to snap to real count.
- Keep the existing non-streaming path as the `else` branch.

**Critical:** Reset `charCount = 0` at the start of each `doComplete` call (important for retries — `retryWithBackoff` calls `doComplete` fresh each attempt so this happens naturally). Do NOT break existing Zod validation — the full response must be accumulated before parsing.
</action>
<verify>
Run `npx tsc --noEmit` — no new type errors (pre-existing runner.ts error is acceptable).
Run `npm test` — all existing tests pass (streaming paths won't be exercised by unit tests but non-streaming paths must remain functional).
Verify that `onToken` is optional in all signatures — existing callers without the callback must continue working unchanged.
</verify>
<done>
Both AnthropicProvider and OpenAICompatibleProvider implement streaming via their respective SDK streaming APIs when an `onToken` callback is provided. Without the callback, the existing non-streaming code path executes unchanged. The `onToken` callback receives estimated token counts during streaming and the authoritative count on completion.
</done>
</task>

<task type="auto">
  <name>Task 2: Wire live progress display with token counter and elapsed timer</name>
  <files>
    src/ui/types.ts
    src/ui/components.ts
    src/ui/renderer.ts
    src/cli/generate.ts
  </files>
  <action>
Connect the `onToken` streaming callback to the terminal UI, add live elapsed time, and update the progress line format to match the locked decision.

**src/ui/types.ts** — Add streaming fields to `RoundDisplayState`:

- Add `streamingTokens?: number` — live token count from streaming callback.
- Add `roundStartMs?: number` — timestamp when round began executing (for live elapsed time computation).
- Add `cumulativeTokens?: number` to `DisplayState` — running total of tokens across all completed + current rounds (for the "(X total)" display).

**src/ui/renderer.ts** — Update spinner tick to compute live elapsed time:

- In `startSpinner()`, inside the `setInterval` callback, after `this.spinnerFrame++`, add a loop over `state.rounds`:
  ```
  for (const [, rd] of state.rounds) {
    if (rd.status === 'running' && rd.roundStartMs) {
      rd.elapsedMs = Date.now() - rd.roundStartMs;
    }
  }
  ```
  This guard ensures cached/done rounds are never overwritten. Place this BEFORE the re-render calls.

**src/ui/components.ts** — Update `renderRoundBlock` running state format:

- In the `case 'running':` branch (the non-retrying else clause), replace the current format with the locked decision format:

  ```
  Round N/T ◆ X,XXX tokens (Y,YYY total) · Zs
  ```

  Where:
  - `N` = `rd.roundNumber`, `T` = total number of rounds in the map (`rounds.size`)
  - The spinner frame character replaces `◆` (use the existing `frame` variable)
  - `X,XXX` = `rd.streamingTokens ?? 0` formatted with `toLocaleString()`
  - `Y,YYY total` = cumulative tokens across all rounds (sum all `rd.tokens` for done rounds + `rd.streamingTokens` for current running round), dimmed
  - `Zs` = elapsed seconds from `rd.elapsedMs`, formatted as `(rd.elapsedMs / 1000).toFixed(1)` + 's'
  - Token count in default color, total in dim, elapsed in dim

- In the `case 'done':` branch, update to the locked decision completion format:

  ```
  ✓ Round N · X,XXX tokens · Zs
  ```

  Where:
  - `X,XXX` = `rd.tokens` (authoritative), formatted via existing `formatTokens()`
  - `Zs` = `rd.elapsedMs` formatted via existing `formatDuration()`
  - Cost display: keep existing cost display for cloud providers after duration
  - This replaces the current "R1 Name · tokens · cost" format. Use "Round N" instead of "RN Name" per locked decision.

- Add a helper function `computeCumulativeTokens(rounds: Map<number, RoundDisplayState>): number` that sums `rd.tokens ?? 0` for done rounds and `rd.streamingTokens ?? 0` for the currently running round.

**src/cli/generate.ts** — Wire onToken callback and set roundStartMs:

- In the `orchestratorEvents.onStepStart` handler, after creating the round display state entry, set `roundStartMs: Date.now()` on the new `RoundDisplayState` object.
- Create a mutable Map to store per-round `onToken` callbacks: `const roundTokenCallbacks = new Map<number, (count: number) => void>()`.
- In `onStepStart`, create the onToken callback for the round:

  ```typescript
  const onToken = (count: number) => {
    const rd = displayState.rounds.get(roundNum);
    if (rd && rd.status === 'running') {
      rd.streamingTokens = count;
    }
  };
  roundTokenCallbacks.set(roundNum, onToken);
  ```

  The spinner interval in TerminalRenderer will pick up `rd.streamingTokens` on its next tick — do NOT call `renderer.onRoundUpdate()` from the onToken callback (that would flood at ~100 tokens/sec).

- In each `createRound*Step` call (rounds 1-6), pass the `onToken` callback to the provider. This requires modifying the round step factories or — more practically — threading it through the `wrapWithCache` wrapper. The simplest approach:
  - Add an `onToken` parameter to `wrapWithCache`: `const wrapWithCache = (roundNum, step, priorRoundNums, onTokenFn?) => ...`
  - Inside `wrapWithCache`'s execute function, after the cache-check block (when the round will actually run), pass `onTokenFn` to the provider call. But since `wrapWithCache` calls `originalExecute(context)` and doesn't directly call the provider, a different approach is needed.
  - **Better approach**: In the `complete()` options at the provider level, the `onRetry` callback is already passed via `makeOnRetry`. Apply the same pattern: each round step's provider call includes an `options` parameter. Since the round step factories (e.g., `createRound1Step`) accept `onRetry` as a parameter and pass it to `provider.complete()`, add `onToken` as an additional parameter to each round step factory.
  - Update each `createRound*Step` call to include the `onToken` callback: pull it from `roundTokenCallbacks.get(roundNum)` at the call site. Since `roundTokenCallbacks` is populated in `onStepStart` which fires BEFORE the step's `execute()` runs, the callback will be available.
  - **Simplest viable approach**: Since the round step factories all call `provider.complete(request, schema, { onRetry })`, and `onToken` needs to be in that options object, add `onToken` alongside `onRetry` in each round factory. Each factory already accepts `onRetry` as a parameter — add `onToken` as an adjacent parameter with the same pattern.

- For the round step factory calls, add `onToken` parameter. For each `createRound*Step(...)` call, add a getter that returns the callback from the Map:

  ```typescript
  () => roundTokenCallbacks.get(roundNum);
  ```

  This lazy getter handles the timing: the callback is populated in onStepStart before execute runs.

- In `onStepComplete`, after setting `rd.status = 'done'` and `rd.tokens`, clear `rd.streamingTokens` (set to undefined) so the live counter doesn't interfere with the final display.
  </action>
  <verify>
  Run `npx tsc --noEmit` — no new type errors.
  Run `npm test` — all tests pass.
  Manually verify with `npm run dev -- generate -v` that the round progress line updates in place with token count and elapsed timer (requires an API key and real repo).
  </verify>
  <done>
  During any active LLM round, the terminal displays a live token counter and elapsed timer that update in place. Format: "Round 3/6 ◆ 1,247 tokens (3,891 total) · 12.3s". Completed rounds display as "✓ Round 3 · 1,247 tokens · 14.2s" and stack visibly. The streaming token count snaps to the authoritative API count on round completion.
  </done>
  </task>

</tasks>

<verification>
1. `npx tsc --noEmit` passes (pre-existing runner.ts error acceptable)
2. `npm test` passes — all existing tests remain green
3. Both provider implementations have a streaming code path gated on `onToken` presence
4. Non-streaming fallback path is preserved in both providers (backward compatible)
5. `RoundDisplayState` has `streamingTokens` and `roundStartMs` fields
6. Spinner tick updates `elapsedMs` only for rounds with `status === 'running'`
7. Progress line format matches locked decision: "Round N/T ◆ X tokens (Y total) · Zs"
8. Completed round format matches locked decision: "✓ Round N · X tokens · Zs"
</verification>

<success_criteria>

- During an active LLM round, the terminal shows a live token counter and elapsed timer updating in place without scrolling (UX-01)
- Completed rounds show final token count and duration in a stacked summary format (UX-01)
- Provider streaming is opt-in via callback — no callback means existing non-streaming behavior
- Both Anthropic and OpenAI-compatible providers implement streaming with token counting
  </success_criteria>

<output>
After completion, create `.planning/phases/05-ux-responsiveness/05-01-SUMMARY.md`
</output>
