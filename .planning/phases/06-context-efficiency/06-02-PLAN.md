---
phase: 06-context-efficiency
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/providers/anthropic.ts
  - src/domain/schemas.ts
  - src/domain/types.ts
  - src/context/tracker.ts
  - src/providers/openai-compat.ts
  - src/providers/base-provider.ts
  - package.json
autonomous: true

must_haves:
  truths:
    - 'Anthropic provider sends system prompt with cache_control ephemeral marker, enabling prompt cache hits on rounds 2-6'
    - 'CompletionResult carries cacheReadTokens and cacheCreationTokens from Anthropic usage response'
    - 'TokenUsageTracker records per-round cache token counts and can compute savings'
    - 'OpenAI-family providers use BPE tokenization via gpt-tokenizer instead of chars/4 heuristic'
  artifacts:
    - path: 'src/providers/anthropic.ts'
      provides: 'cache_control on system prompt TextBlockParam, cache usage extraction from response'
      contains: 'cache_control'
    - path: 'src/domain/schemas.ts'
      provides: 'UsageSchema extended with optional cacheReadTokens and cacheCreationTokens'
      contains: 'cacheReadTokens'
    - path: 'src/context/tracker.ts'
      provides: 'TokenUsageTracker extended with cache token recording and savings computation'
      contains: 'cacheReadTokens'
    - path: 'src/providers/openai-compat.ts'
      provides: 'BPE tokenization override using gpt-tokenizer'
      contains: 'countTokens'
  key_links:
    - from: 'src/providers/anthropic.ts'
      to: 'src/domain/schemas.ts'
      via: 'CompletionResult.usage carries cache fields'
      pattern: 'cacheReadTokens'
    - from: 'src/context/tracker.ts'
      to: 'src/domain/types.ts'
      via: 'TokenUsage extended with cache fields'
      pattern: 'cacheReadTokens'
    - from: 'src/providers/openai-compat.ts'
      to: 'gpt-tokenizer'
      via: 'countTokens import for BPE estimation'
      pattern: 'gpt-tokenizer'
---

<objective>
Add Anthropic prompt caching to reduce token costs on rounds 2-6, install gpt-tokenizer for accurate OpenAI token counting, and extend the Usage/tracker pipeline to carry cache savings data.

Purpose: Anthropic prompt caching lets rounds 2-6 reuse the cached system prompt (90% cost reduction on input tokens). BPE tokenization eliminates the 15-25% counting error on OpenAI providers, preventing context window overflows. Both are foundational for the per-round savings display in Plan 03. Addresses EFF-02 and EFF-05.

Output: Anthropic provider with cache_control blocks, extended Usage schema with cache fields, tracker with savings computation, OpenAI provider with BPE tokenization.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@src/providers/anthropic.ts
@src/providers/openai-compat.ts
@src/providers/base-provider.ts
@src/domain/schemas.ts
@src/domain/types.ts
@src/context/tracker.ts
@src/context/types.ts
@.planning/phases/06-context-efficiency/06-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Anthropic prompt caching, Usage schema extension, and tracker cache fields</name>
  <files>src/providers/anthropic.ts, src/domain/schemas.ts, src/domain/types.ts, src/context/tracker.ts, src/context/types.ts</files>
  <action>
**In `src/domain/schemas.ts`:**

Extend `UsageSchema` with optional cache token fields:

```typescript
export const UsageSchema = z.object({
  inputTokens: z.number().int().min(0),
  outputTokens: z.number().int().min(0),
  cacheReadTokens: z.number().int().min(0).optional(),
  cacheCreationTokens: z.number().int().min(0).optional(),
});
```

No change needed in `src/domain/types.ts` — `Usage` is derived via `z.infer` and will pick up the new fields automatically.

**In `src/context/types.ts`:**

Extend `TokenUsageSchema` with optional cache fields:

```typescript
export const TokenUsageSchema = z.object({
  round: z.number(),
  inputTokens: z.number(),
  outputTokens: z.number(),
  contextTokens: z.number(),
  fileContentTokens: z.number(),
  budgetTokens: z.number(),
  cacheReadTokens: z.number().optional(),
  cacheCreationTokens: z.number().optional(),
});
```

**In `src/context/tracker.ts`:**

1. Add a `getCacheReadPerMillion()` static method that returns the cache read pricing rate. Anthropic cache reads cost 0.10x base input price:

```typescript
private static readonly CACHE_READ_MULTIPLIER = 0.1;
private static readonly CACHE_WRITE_MULTIPLIER = 1.25;
```

Add these as class-level constants near `MODEL_COSTS`.

2. Add a method to compute per-round savings from cache:

```typescript
/**
 * Compute cache savings for a specific round.
 * Returns null if no cache data for this round.
 */
getRoundCacheSavings(roundNumber: number): {
  tokensSaved: number;
  dollarsSaved: number;
  percentSaved: number;
} | null {
  const usage = this.rounds.find((r) => r.round === roundNumber);
  if (!usage?.cacheReadTokens) return null;

  const costs = TokenUsageTracker.MODEL_COSTS[this.model] ?? TokenUsageTracker.MODEL_COSTS['default'];
  const cacheRead = usage.cacheReadTokens;

  // Tokens saved = cache reads (those weren't re-processed at full cost)
  const tokensSaved = cacheRead;

  // Dollar savings = cacheRead * (fullRate - cacheReadRate) / 1M
  const fullCostPerToken = costs.inputPerMillion / 1_000_000;
  const cacheReadCostPerToken = fullCostPerToken * TokenUsageTracker.CACHE_READ_MULTIPLIER;
  const dollarsSaved = cacheRead * (fullCostPerToken - cacheReadCostPerToken);

  // Percentage saved relative to total input (input + cacheRead + cacheCreation)
  const totalInput = usage.inputTokens + cacheRead + (usage.cacheCreationTokens ?? 0);
  const percentSaved = totalInput > 0 ? tokensSaved / totalInput : 0;

  return { tokensSaved, dollarsSaved, percentSaved };
}
```

3. Update `estimateCost()` to account for cache token pricing when cache fields are present:

```typescript
estimateCost(inputTokens: number, outputTokens: number, cacheReadTokens?: number, cacheCreationTokens?: number): number {
  const costs = TokenUsageTracker.MODEL_COSTS[this.model] ?? TokenUsageTracker.MODEL_COSTS['default'];
  let total = (inputTokens / 1_000_000) * costs.inputPerMillion +
    (outputTokens / 1_000_000) * costs.outputPerMillion;

  if (cacheReadTokens) {
    total += (cacheReadTokens / 1_000_000) * costs.inputPerMillion * TokenUsageTracker.CACHE_READ_MULTIPLIER;
  }
  if (cacheCreationTokens) {
    total += (cacheCreationTokens / 1_000_000) * costs.inputPerMillion * TokenUsageTracker.CACHE_WRITE_MULTIPLIER;
  }

  return total;
}
```

4. Update `getRoundCost()` and `getTotalCost()` to pass cache fields to `estimateCost()`:

```typescript
getRoundCost(roundNumber: number): number {
  const usage = this.rounds.find((r) => r.round === roundNumber);
  if (!usage) return 0;
  return this.estimateCost(usage.inputTokens, usage.outputTokens, usage.cacheReadTokens, usage.cacheCreationTokens);
}

getTotalCost(): number {
  let total = 0;
  for (const r of this.rounds) {
    total += this.estimateCost(r.inputTokens, r.outputTokens, r.cacheReadTokens, r.cacheCreationTokens);
  }
  return total;
}
```

**In `src/providers/anthropic.ts`:**

1. Import `TextBlockParam` type:

   ```typescript
   import type { TextBlockParam } from '@anthropic-ai/sdk/resources/messages.js';
   ```

2. In `doComplete()`, change the `system` field from a plain string to a `TextBlockParam[]` array with `cache_control`:

   Replace:

   ```typescript
   system: request.systemPrompt,
   ```

   With:

   ```typescript
   system: [
     {
       type: 'text' as const,
       text: request.systemPrompt,
       cache_control: { type: 'ephemeral' as const },
     },
   ] satisfies TextBlockParam[],
   ```

   Use `satisfies` (not `as`) for type safety. If the installed SDK types don't support `satisfies` on `TextBlockParam` with `cache_control`, fall back to building the array separately:

   ```typescript
   const systemBlocks: TextBlockParam[] = [
     {
       type: 'text',
       text: request.systemPrompt,
       cache_control: { type: 'ephemeral' },
     },
   ];
   ```

   Then use `system: systemBlocks` in the params object.

3. In BOTH the streaming and non-streaming return paths, extract cache usage from the response and include it in the `usage` field:

   For the streaming path (after `const message = await stream.finalMessage()`):

   ```typescript
   return {
     data,
     usage: {
       inputTokens: message.usage.input_tokens,
       outputTokens: message.usage.output_tokens,
       cacheReadTokens:
         ((message.usage as Record<string, unknown>).cache_read_input_tokens as
           | number
           | undefined) ?? undefined,
       cacheCreationTokens:
         ((message.usage as Record<string, unknown>).cache_creation_input_tokens as
           | number
           | undefined) ?? undefined,
     },
     model: message.model,
     duration,
   };
   ```

   For the non-streaming path (after `const response = await this.client.messages.create(params)`):
   Same pattern — extract `cache_read_input_tokens` and `cache_creation_input_tokens` from `response.usage`.

   Note: The SDK types may already include these fields on the `Usage` type (verified present in 0.39.0). If they are directly accessible, use `message.usage.cache_read_input_tokens ?? undefined` without the cast. Check the actual type at implementation time and use the simpler form if available.
   </action>
   <verify>
   Run `npx tsc --noEmit` — no type errors. Run `npm test` — existing tests pass. Confirm that `UsageSchema` includes `cacheReadTokens` and `cacheCreationTokens` as optional fields. Confirm `AnthropicProvider.doComplete()` sends `system` as an array with `cache_control`.
   </verify>
   <done>
   Anthropic provider sends system prompt with `cache_control: { type: 'ephemeral' }`, enabling automatic prompt cache hits on rounds 2-6. CompletionResult carries `cacheReadTokens` and `cacheCreationTokens` from the response. TokenUsageTracker records cache fields and computes per-round cache savings. Cost estimation accounts for cache read (0.1x) and creation (1.25x) pricing.
   </done>
   </task>

<task type="auto">
  <name>Task 2: Install gpt-tokenizer and add BPE token estimation to OpenAI provider</name>
  <files>package.json, src/providers/openai-compat.ts, src/providers/base-provider.ts</files>
  <action>
**Install gpt-tokenizer:**

```bash
npm install gpt-tokenizer
```

This adds `gpt-tokenizer` (^3.4.0) as a production dependency. It is pure TypeScript, synchronous, and supports ESM (no WASM, no native bindings).

**In `src/providers/openai-compat.ts`:**

1. Add imports at the top of the file:

   ```typescript
   import { countTokens } from 'gpt-tokenizer';
   import { countTokens as countTokensCl100k } from 'gpt-tokenizer/encoding/cl100k_base';
   ```

2. Add an `estimateTokens()` override to the `OpenAICompatibleProvider` class:

   ```typescript
   /**
    * BPE token estimation using gpt-tokenizer (EFF-05).
    * Replaces the chars/4 heuristic with accurate encoding-aware counting.
    * Uses o200k_base for modern models (gpt-4o, gpt-4.1, o-series),
    * cl100k_base for legacy models (gpt-4, gpt-3.5-turbo).
    */
   override estimateTokens(text: string): number {
     if (this.model.startsWith('gpt-4-') || this.model.startsWith('gpt-3.5-')) {
       return countTokensCl100k(text);
     }
     return countTokens(text);
   }
   ```

   This overrides the `estimateTokens()` method from `BaseProvider` (which uses `Math.ceil(text.length / 4)`). The override only applies to `OpenAICompatibleProvider` instances — `AnthropicProvider` inherits the base heuristic (acceptable because Anthropic returns actual token counts in usage responses; estimation is only for budget planning).

   Place the method after the constructor and before `doComplete()`.

**Do NOT modify `src/providers/base-provider.ts`** — the base class `estimateTokens()` remains as the fallback for non-OpenAI providers.
</action>
<verify>
Run `npx tsc --noEmit` — no type errors. Run `npm test` — existing tests pass. Verify `gpt-tokenizer` is in `package.json` dependencies. Verify `OpenAICompatibleProvider` has an `estimateTokens` override.
</verify>
<done>
`gpt-tokenizer` installed as production dependency. `OpenAICompatibleProvider.estimateTokens()` uses BPE tokenization (o200k_base for modern models, cl100k_base for legacy), eliminating the 15-25% counting error from the chars/4 heuristic. `AnthropicProvider` and other providers still use the base heuristic (acceptable for budget planning when providers return actual counts).
</done>
</task>

</tasks>

<verification>
1. `npx tsc --noEmit` passes with no type errors
2. `npm test` passes — all existing tests green
3. `UsageSchema` has optional `cacheReadTokens` and `cacheCreationTokens` fields
4. `AnthropicProvider.doComplete()` sends system as `TextBlockParam[]` with `cache_control`
5. `TokenUsageTracker` has `getRoundCacheSavings()` method and cache-aware cost estimation
6. `gpt-tokenizer` is in `package.json` dependencies
7. `OpenAICompatibleProvider` has `estimateTokens` override using `countTokens`
</verification>

<success_criteria>

- Anthropic provider sends cache_control on system prompt — rounds 2-6 get cache hits
- CompletionResult carries cache token fields through to the tracker
- Cost estimation correctly applies 0.1x rate for cache reads and 1.25x for cache creation
- OpenAI-family providers use BPE tokenization for accurate budget planning
- No breaking changes to non-Anthropic providers
  </success_criteria>

<output>
After completion, create `.planning/phases/06-context-efficiency/06-02-SUMMARY.md`
</output>
