---
phase: 12-vector-storage-foundation
plan: 03
type: execute
wave: 2
depends_on: ['12-01', '12-02']
files_modified:
  - src/vector/embedder.ts
  - src/vector/reindex.ts
  - src/cli/reindex.ts
  - src/cli/index.ts
autonomous: true
user_setup:
  - service: openai
    why: 'Embedding API for generating text-embedding-3-small vectors'
    env_vars:
      - name: OPENAI_API_KEY
        source: 'OpenAI Dashboard -> API keys -> Create new secret key'

must_haves:
  truths:
    - 'User runs `handover reindex` and sees progress bar showing chunks processed across 14 documents'
    - 'User finds .handover/search.db populated with document chunks and embeddings after reindex'
    - "User runs `handover reindex` again with unchanged docs and sees 'Skipped N unchanged documents' message"
    - 'User changes one document, runs reindex, and only the changed document is re-embedded'
    - 'User runs `handover reindex` without prior `handover generate` and sees clear error message'
  artifacts:
    - path: 'src/vector/embedder.ts'
      provides: 'EmbeddingProvider class for calling OpenAI embedding API with batching and rate limiting'
      exports: ['EmbeddingProvider', 'createEmbeddingProvider']
    - path: 'src/vector/reindex.ts'
      provides: 'Reindex orchestrator: reads docs, fingerprints, chunks, embeds, stores with progress'
      exports: ['reindexDocuments']
    - path: 'src/cli/reindex.ts'
      provides: 'CLI handler for `handover reindex` command'
      exports: ['runReindex']
    - path: 'src/cli/index.ts'
      provides: 'Registers `handover reindex` command with commander'
      contains: 'reindex'
  key_links:
    - from: 'src/vector/reindex.ts'
      to: 'src/vector/vector-store.ts'
      via: 'opens VectorStore, inserts chunks+embeddings, upserts fingerprints'
      pattern: 'VectorStore'
    - from: 'src/vector/reindex.ts'
      to: 'src/vector/chunker.ts'
      via: 'calls chunkDocument() on each markdown file'
      pattern: 'chunkDocument'
    - from: 'src/vector/reindex.ts'
      to: 'src/vector/embedder.ts'
      via: 'calls embedBatch() to generate embedding vectors'
      pattern: 'embedBatch'
    - from: 'src/cli/reindex.ts'
      to: 'src/vector/reindex.ts'
      via: 'calls reindexDocuments() orchestrator function'
      pattern: 'reindexDocuments'
    - from: 'src/cli/index.ts'
      to: 'src/cli/reindex.ts'
      via: 'registers reindex command in commander program'
      pattern: 'reindex'
---

<objective>
Wire the full reindex pipeline: embedding provider for OpenAI API calls, reindex orchestrator that reads generated documents + chunks + embeds + stores with content-hash change detection, progress bar UI, and the `handover reindex` CLI command.

Purpose: This plan connects the database (Plan 01) and chunker (Plan 02) into a working end-to-end pipeline that the user invokes via `handover reindex`. Delivers the core user-facing experience: generated docs become searchable embeddings.

Output: Working `handover reindex` command that chunks and embeds all 14 documents into `.handover/search.db` with progress reporting and change detection.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/12-vector-storage-foundation/12-01-SUMMARY.md
@.planning/phases/12-vector-storage-foundation/12-02-SUMMARY.md
@src/providers/base-provider.ts
@src/providers/factory.ts
@src/config/schema.ts
@src/cli/index.ts
@src/cli/generate.ts
@src/cache/round-cache.ts
@src/utils/logger.ts
@src/utils/errors.ts
@package.json
</context>

<tasks>

<task type="auto">
  <name>Task 1: Embedding provider with batch API and rate limiting</name>
  <files>
    src/vector/embedder.ts
  </files>
  <action>
**Install dependency:**

```bash
npm install cli-progress
npm install --save-dev @types/cli-progress
```

cli-progress is for the progress bar in Task 2, but install it now to avoid two npm install runs.

**Create `src/vector/embedder.ts`:**

`EmbeddingProvider` class — does NOT extend BaseProvider (BaseProvider is designed for LLM completions with Zod schema validation, which is irrelevant for embeddings). Instead, implement a focused embedding-specific class with its own retry logic:

```typescript
import { retryWithBackoff } from '../utils/rate-limiter.js';
import { logger } from '../utils/logger.js';
```

Constructor takes: `{ model: string; apiKey: string; batchSize?: number; }`. Defaults: batchSize=100.

`async embed(texts: string[]): Promise<number[][]>` — Core method. Calls OpenAI embedding API for a single batch of texts. Uses `fetch()` to call `https://api.openai.com/v1/embeddings` with:

- `input: texts`
- `model: this.model`
- Headers: `Authorization: Bearer ${this.apiKey}`, `Content-Type: application/json`

Parse response, extract embeddings from `data[].embedding`. Wrap in `retryWithBackoff` from `src/utils/rate-limiter.ts` with:

- maxRetries: 3
- baseDelayMs: 30_000
- isRetryable: status 429 or >= 500

Return `number[][]` (one embedding vector per input text).

`async embedBatch(texts: string[]): Promise<{ embeddings: number[][]; totalTokens: number; dimensions: number; }>` — Splits texts into batches of `this.batchSize`, calls `embed()` for each batch, concatenates results. Tracks total_tokens from API responses. Returns the embedding dimension from the first result.

`getDimensions(): number` — Returns the expected dimension count for the configured model. Use the `EMBEDDING_MODELS` map from `src/vector/types.ts`. Default to 1536 if model not found.

**Factory function:**

`createEmbeddingProvider(config: HandoverConfig): EmbeddingProvider` — Resolves API key and model from config:

1. If `config.embedding` is set, use `config.embedding.model` and resolve API key from `config.embedding.apiKeyEnv ?? 'OPENAI_API_KEY'`
2. If `config.embedding` is not set but `config.provider === 'openai'`, reuse the OpenAI API key from the main provider config
3. If no OpenAI API key found, throw a clear error: "Embedding requires an OpenAI API key. Set OPENAI_API_KEY or configure embedding.apiKeyEnv in .handover.yml"

**Important:** All logging goes to stderr via `logger.log()`, never to stdout. This is critical for future MCP server compatibility.
</action>
<verify>
Run `npx tsc --noEmit` to verify type correctness. Verify the factory function resolves API keys correctly by tracing the logic paths. The actual API call cannot be tested without a real key — integration testing deferred to manual verification.
</verify>
<done>
EmbeddingProvider class can batch-embed text arrays via OpenAI API with retry logic. Factory function resolves API key from config with clear error on missing key. All logging to stderr.
</done>
</task>

<task type="auto">
  <name>Task 2: Reindex orchestrator with change detection, progress bar, and CLI command</name>
  <files>
    src/vector/reindex.ts
    src/cli/reindex.ts
    src/cli/index.ts
  </files>
  <action>
**1. Create `src/vector/reindex.ts`:**

This is the main orchestrator that connects chunker, embedder, and vector store.

`reindexDocuments(options: ReindexOptions): Promise<ReindexResult>` — Main entry point.

```typescript
interface ReindexOptions {
  config: HandoverConfig;
  outputDir: string; // Where generated docs live (default: ./handover)
  verbose?: boolean;
  onProgress?: (event: ReindexProgressEvent) => void;
}

interface ReindexProgressEvent {
  phase: 'scanning' | 'chunking' | 'embedding' | 'storing' | 'complete';
  documentsTotal: number;
  documentsProcessed: number;
  documentsSkipped: number;
  chunksTotal: number;
  chunksProcessed: number;
}

interface ReindexResult {
  documentsProcessed: number;
  documentsSkipped: number;
  chunksCreated: number;
  totalTokens: number;
  embeddingModel: string;
  embeddingDimensions: number;
}
```

**Orchestration flow:**

1. **Validate output directory exists**: Check that `outputDir` contains generated markdown files (glob for `*.md`). If no files found, throw an error: "No generated documents found in {outputDir}. Run `handover generate` first."

2. **Discover documents**: Read all `.md` files from `outputDir`. For each file, determine:
   - `sourceFile`: filename (e.g., "03-ARCHITECTURE.md")
   - `docId`: derived from filename (e.g., "03-architecture")
   - `docType`: derived from filename (e.g., "architecture") — use a mapping based on the known 14 document types from the registry, or fall back to "unknown"

3. **Open vector store**: Create VectorStore with config derived from HandoverConfig embedding section. Open the database (creates if needed, validates dimensions).

4. **Create embedding provider**: Use `createEmbeddingProvider(config)`.

5. **Change detection loop**: For each document:
   - Compute content-hash fingerprint: `SHA-256(JSON.stringify({ sourceFile, content }))`
   - Check stored fingerprint via `vectorStore.getDocumentFingerprint(docId)`
   - If fingerprint matches: skip (increment documentsSkipped counter), emit progress event
   - If fingerprint differs or not found: proceed to chunk + embed

6. **Chunk changed documents**: Call `chunkDocument(content, { sourceFile, docId, docType })` for each changed document. Collect all chunks.

7. **Embed all chunks in batches**: Extract text content from chunks, call `embeddingProvider.embedBatch(texts)`. The progress callback fires after each batch.

8. **Store in database**: For each changed document:
   - Delete old chunks: `vectorStore.deleteDocumentChunks(docId)`
   - Insert new chunks + embeddings: `vectorStore.insertChunks(docChunks, docEmbeddings)`
   - Update fingerprint: `vectorStore.upsertDocumentFingerprint({ docId, fingerprint, indexedAt: now, chunkCount })`

9. **Close and return**: Close vector store. Return ReindexResult with stats.

**2. Create `src/cli/reindex.ts`:**

CLI handler for `handover reindex`.

```typescript
import cliProgress from 'cli-progress';

export interface ReindexCommandOptions {
  verbose?: boolean;
  force?: boolean;  // Bypass change detection, re-embed everything
}

export async function runReindex(options: ReindexCommandOptions): Promise<void> {
```

Flow:

1. Load config via `loadConfig()`
2. Resolve output directory from config
3. Set up progress bar using cli-progress SingleBar:
   - Format: `Reindexing | {bar} | {percentage}% | {value}/{total} chunks | {documentsProcessed}/{documentsTotal} docs`
   - Use `cliProgress.Presets.shades_classic`
4. Call `reindexDocuments()` with progress callback that updates the bar
5. On completion, stop the bar and print summary:
   - "Reindexed N documents (M chunks, K tokens), skipped P unchanged"
   - If all skipped: "All N documents unchanged, nothing to reindex"
6. Handle errors with `handleCliError()` from `src/utils/errors.ts`

If `--force` flag is set, pass a flag to reindexDocuments that skips change detection (re-embeds everything). Implement this as: before the change detection loop, delete all fingerprints so every document appears "changed".

**3. Update `src/cli/index.ts`:**

Add the reindex command to the Commander program:

```typescript
program
  .command('reindex')
  .description('Build or update vector search index from generated documentation')
  .option('--force', 'Re-embed all documents (ignore change detection)')
  .option('-v, --verbose', 'Show detailed output')
  .action(async (opts) => {
    const { runReindex } = await import('./reindex.js');
    await runReindex(opts);
  });
```

Use the lazy import pattern consistent with the existing `analyze` and `estimate` commands.

**Important implementation details:**

- Content hash uses `createHash('sha256')` from `node:crypto` — same pattern as `RoundCache.computeAnalysisFingerprint()` and `hashContent()` in `src/analyzers/cache.ts`
- All logging to stderr via logger, never stdout
- Progress bar writes to stderr (cli-progress defaults to stderr when `stream` option is not set, but explicitly set `stream: process.stderr` to be safe)
- If the embedding API call fails for a document, log the error and continue with remaining documents (graceful degradation, consistent with handover's error handling strategy)
- The `INDEX.md` file (00-INDEX.md) should be excluded from embedding — it's a generated index, not content
  </action>
  <verify>

1. `npx tsc --noEmit` passes with zero errors
2. `npm test` passes (no regressions)
3. `node --loader tsx src/cli/index.ts reindex --help` shows the reindex command with --force and --verbose options
4. Manual test with a real OPENAI_API_KEY:
   - Run `handover reindex` — should embed documents with progress bar
   - Run `handover reindex` again — should show "Skipped N unchanged documents"
   - Modify one output file, run `handover reindex` — should only re-embed the modified file
5. Run `handover reindex` without prior `handover generate` — should show clear error
   </verify>
   <done>
   `handover reindex` command works end-to-end: reads generated docs, chunks with markdown-aware splitting, embeds via OpenAI API with batching, stores in SQLite with vec0, shows progress bar, skips unchanged documents via content-hash detection. All 6 STORE requirements are satisfied.
   </done>
   </task>

</tasks>

<verification>
1. `npx tsc --noEmit` passes
2. `npm test` passes (no regressions in 254 existing tests)
3. `handover reindex --help` shows correct options
4. Full integration test: `handover generate` then `handover reindex` produces `.handover/search.db`
5. Change detection: second `handover reindex` skips unchanged docs
6. Missing docs: `handover reindex` in empty dir shows clear error
7. Progress bar shows chunk-level progress during embedding
</verification>

<success_criteria>

- EmbeddingProvider calls OpenAI API with batching and retry
- Reindex orchestrator connects chunker + embedder + vector store
- Content-hash change detection skips unchanged documents
- Progress bar shows real-time chunk processing progress
- `handover reindex` CLI command registered and functional
- Graceful error handling for missing docs, missing API key, API failures
- All logging to stderr (MCP-safe)
  </success_criteria>

<output>
After completion, create `.planning/phases/12-vector-storage-foundation/12-03-SUMMARY.md`
</output>
