---
phase: 22-gemini-provider
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - package.json
  - src/providers/gemini.ts
  - src/vector/gemini-embedder.ts
autonomous: true

must_haves:
  truths:
    - "GeminiProvider extends BaseProvider and produces structured JSON output via responseSchema"
    - "GeminiEmbeddingProvider implements EmbeddingClient with 1536-dimension output for index compatibility"
    - "@google/genai SDK is installed and importable"
  artifacts:
    - path: "src/providers/gemini.ts"
      provides: "GeminiProvider class for chat completion"
      exports: ["GeminiProvider"]
    - path: "src/vector/gemini-embedder.ts"
      provides: "GeminiEmbeddingProvider class for embedding"
      exports: ["GeminiEmbeddingProvider"]
  key_links:
    - from: "src/providers/gemini.ts"
      to: "src/providers/base-provider.ts"
      via: "extends BaseProvider"
      pattern: "extends BaseProvider"
    - from: "src/providers/gemini.ts"
      to: "src/providers/schema-utils.ts"
      via: "zodToToolSchema for responseSchema"
      pattern: "zodToToolSchema"
    - from: "src/vector/gemini-embedder.ts"
      to: "src/vector/embedder.ts"
      via: "implements EmbeddingClient interface"
      pattern: "implements EmbeddingClient"
---

<objective>
Implement the GeminiProvider (LLM chat completion) and GeminiEmbeddingProvider (embedding) as new files using the `@google/genai` SDK.

Purpose: These are the core runtime classes that execute Gemini API calls for all 6 analysis rounds and for embedding generation during reindex/search.

Output: Two new source files (`gemini.ts`, `gemini-embedder.ts`) and `@google/genai` installed in package.json.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

@src/providers/base-provider.ts
@src/providers/anthropic.ts
@src/providers/schema-utils.ts
@src/vector/embedder.ts
@src/utils/rate-limiter.ts
@src/utils/errors.ts
@src/domain/types.ts
@.planning/phases/22-gemini-provider/22-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install @google/genai SDK and create GeminiProvider</name>
  <files>package.json, src/providers/gemini.ts</files>
  <action>
1. Install the official Google Gen AI SDK:
   ```bash
   npm install @google/genai
   ```
   IMPORTANT: Use `@google/genai` (NOT `@google/generative-ai` which is deprecated and EOL Nov 2025).

2. Create `src/providers/gemini.ts` implementing `GeminiProvider extends BaseProvider`:

   - Import `GoogleGenAI` from `@google/genai`
   - Import `BaseProvider` from `./base-provider.js`
   - Import `zodToToolSchema` from `./schema-utils.js`
   - Import `ProviderError` from `../utils/errors.js`
   - Import types: `CompletionRequest`, `CompletionResult` from `../domain/types.js`, `z` from `zod`

   Class member (REQUIRED — satisfies `abstract readonly name: string` in BaseProvider):
   - `readonly name = 'gemini'` as the first class member, matching the pattern in AnthropicProvider (line 15 of anthropic.ts).

   Constructor:
   - `constructor(apiKey: string, model: string = 'gemini-2.5-flash', concurrency: number = 4)`
   - Call `super(model, concurrency)`
   - Create `this.client = new GoogleGenAI({ apiKey })`
   - Call `this.logInit('Gemini', concurrency)`

   `doComplete<T>()` implementation:
   - Convert schema to JSON Schema via `zodToToolSchema(schema)`
   - Call `this.client.models.generateContent()` with:
     - `model: this.model`
     - `contents: [{ role: 'user', parts: [{ text: request.userPrompt }] }]`
     - `config.systemInstruction: request.systemPrompt`
     - `config.responseMimeType: 'application/json'`
     - `config.responseSchema: responseSchema as Record<string, unknown>`
     - `config.maxOutputTokens: request.maxTokens ?? 4096`
     - `config.temperature: request.temperature ?? 0.7`

   Safety filter handling (CRITICAL — check BEFORE throwing generic error):
   - If `response.text` is falsy, check `response.candidates?.[0]?.finishReason`
   - If `finishReason === 'SAFETY'` or `finishReason === 'RECITATION'`:
     throw `ProviderError('Gemini safety filter blocked the response', ...)` with code `'PROVIDER_SAFETY_BLOCKED'`
   - Otherwise throw `ProviderError('No structured response from Gemini model', ...)` with code `'PROVIDER_NO_RESPONSE'`

   Successful response parsing:
   - `const data = schema.parse(JSON.parse(response.text))`
   - Extract usage from `response.usageMetadata` (promptTokenCount, candidatesTokenCount)
   - If `onToken` callback provided and `candidatesTokenCount` exists, call `onToken(candidatesTokenCount)`
   - Return `{ data, usage: { inputTokens, outputTokens }, model: this.model, duration }`

   `isRetryable()`:
   - Check `err.status` for 429, 500, 503 — return true for these
   - Otherwise return false

   `maxContextTokens()`:
   - Return `1_000_000` (Gemini 2.5 Flash 1M context window)
  </action>
  <verify>
    Run `npx tsc --noEmit` — no TypeScript errors.
    Verify `@google/genai` appears in package.json dependencies.
    Verify `import { GoogleGenAI } from '@google/genai'` resolves.
  </verify>
  <done>
    `src/providers/gemini.ts` exports `GeminiProvider` extending `BaseProvider`.
    `readonly name = 'gemini'` satisfies `abstract readonly name: string` from BaseProvider.
    It uses `responseMimeType: 'application/json'` + `responseSchema` for structured output.
    Safety filter check produces actionable `ProviderError` with `PROVIDER_SAFETY_BLOCKED` code.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create GeminiEmbeddingProvider</name>
  <files>src/vector/gemini-embedder.ts</files>
  <action>
Create `src/vector/gemini-embedder.ts` implementing `GeminiEmbeddingProvider`:

- Import `GoogleGenAI` from `@google/genai`
- Import `retryWithBackoff` from `../utils/rate-limiter.js`
- Import `HandoverError` from `../utils/errors.js`
- Import `EmbeddingBatchResult`, `EmbeddingClient` from `./embedder.js`

Define constant: `const GEMINI_EMBEDDING_DIMENSIONS = 1536`
(CRITICAL: Must be 1536, NOT 3072 — forces index compatibility with existing OpenAI-based indexes via `outputDimensionality` parameter)

Class `GeminiEmbeddingProvider implements EmbeddingClient`:
- `readonly provider = 'remote' as const`
- `readonly model: string`
- `private client: GoogleGenAI`
- `private readonly batchSize: number`

Constructor:
- `constructor(apiKey: string, model = 'gemini-embedding-001', batchSize = 100)`
- Create `this.client = new GoogleGenAI({ apiKey })`

`embedBatch(texts: string[])`:
- Return early for empty array: `{ embeddings: [], totalTokens: 0, dimensions: GEMINI_EMBEDDING_DIMENSIONS }`
- Process texts in batches of `this.batchSize`
- For each batch, wrap in `retryWithBackoff` with:
  - `maxRetries: 3`, `baseDelayMs: 30_000`
  - `isRetryable`: check `err.status` for 429 or >= 500
- Within the retry function, loop over each text in the batch and call:
  ```
  this.client.models.embedContent({
    model: this.model,
    contents: text,
    config: { outputDimensionality: GEMINI_EMBEDDING_DIMENSIONS },
  })
  ```
  (One call per text — avoids ambiguity in response shape for batch calls per research recommendation)
- Extract values from `response.embeddings?.[0]?.values ?? response.embedding?.values`
- If no values found, throw `HandoverError` with code `'EMBEDDING_EMPTY_RESPONSE'`
- Collect all embeddings into `allEmbeddings` array

Token estimation: `texts.reduce((sum, t) => sum + Math.ceil(t.length / 4), 0)` (same heuristic as existing OpenAI embedder)

Return: `{ embeddings: allEmbeddings, totalTokens, dimensions: GEMINI_EMBEDDING_DIMENSIONS }`

`getDimensions()`:
- Return `GEMINI_EMBEDDING_DIMENSIONS` (1536)
  </action>
  <verify>
    Run `npx tsc --noEmit` — no TypeScript errors.
    Verify `GeminiEmbeddingProvider` implements `EmbeddingClient` interface (embedBatch + getDimensions).
    Verify `getDimensions()` returns 1536.
  </verify>
  <done>
    `src/vector/gemini-embedder.ts` exports `GeminiEmbeddingProvider` implementing `EmbeddingClient`.
    Embedding dimensions are forced to 1536 via `outputDimensionality` for index compatibility.
    Retry logic handles 429 and 5xx errors with 30-second backoff.
  </done>
</task>

</tasks>

<verification>
- `npx tsc --noEmit` passes with zero errors
- `@google/genai` is in package.json dependencies (not `@google/generative-ai`)
- `src/providers/gemini.ts` exists and exports `GeminiProvider`
- `src/vector/gemini-embedder.ts` exists and exports `GeminiEmbeddingProvider`
- Both files import from `@google/genai` (correct package)
- GeminiProvider uses `responseMimeType: 'application/json'` and `responseSchema` (not function calling)
- GeminiEmbeddingProvider uses `outputDimensionality: 1536` (not default 3072)
</verification>

<success_criteria>
- TypeScript compiles without errors
- GeminiProvider can be instantiated with (apiKey, model, concurrency)
- GeminiEmbeddingProvider can be instantiated with (apiKey, model, batchSize)
- Both classes follow the same patterns as AnthropicProvider and EmbeddingProvider respectively
</success_criteria>

<output>
After completion, create `.planning/phases/22-gemini-provider/22-01-SUMMARY.md`
</output>
