---
phase: 05-ai-analysis-rounds
plan: 02
type: execute
wave: 2
depends_on: ["05-01"]
files_modified:
  - src/ai-rounds/runner.ts
  - src/ai-rounds/round-1-overview.ts
  - src/ai-rounds/round-2-modules.ts
  - src/ai-rounds/fallbacks.ts
autonomous: true

must_haves:
  truths:
    - "Round execution engine calls LLMProvider.complete() with round schema, validates claims, checks quality, and retries once on failure"
    - "Round 1 produces a project overview interleaving business purpose with technical landscape from static analysis data"
    - "Round 2 identifies module boundaries from AST imports, directory structure, and naming patterns"
    - "Failed rounds fall back to raw static analysis data with degraded status marker"
    - "Retry happens at most once (boolean flag) per round for either validation failure (>30% drop rate) or quality failure"
  artifacts:
    - path: "src/ai-rounds/runner.ts"
      provides: "Round execution engine with validation, quality check, retry, and fallback"
      exports: ["executeRound"]
    - path: "src/ai-rounds/round-1-overview.ts"
      provides: "Round 1 Project Overview step creator"
      exports: ["createRound1Step"]
    - path: "src/ai-rounds/round-2-modules.ts"
      provides: "Round 2 Module Detection step creator"
      exports: ["createRound2Step"]
    - path: "src/ai-rounds/fallbacks.ts"
      provides: "Static analysis fallback builders for all 6 rounds"
      exports: ["buildRound1Fallback", "buildRound2Fallback", "buildRound3Fallback", "buildRound4Fallback", "buildRound5Fallback", "buildRound6Fallback"]
  key_links:
    - from: "src/ai-rounds/runner.ts"
      to: "src/providers/base.ts"
      via: "LLMProvider.complete() call"
      pattern: "provider\\.complete"
    - from: "src/ai-rounds/runner.ts"
      to: "src/context/compressor.ts"
      via: "compressRoundOutput() for inter-round context"
      pattern: "compressRoundOutput"
    - from: "src/ai-rounds/runner.ts"
      to: "src/ai-rounds/validator.ts"
      via: "validateRoundClaims() call"
      pattern: "validateRoundClaims"
    - from: "src/ai-rounds/round-1-overview.ts"
      to: "src/orchestrator/step.ts"
      via: "createStep() for DAG integration"
      pattern: "createStep"
---

<objective>
Build the round execution engine and implement Rounds 1-2 (the sequential foundation rounds that all subsequent rounds depend on), plus static fallback builders for graceful degradation.

Purpose: The execution engine (`runner.ts`) is the core loop that every round uses: call LLM, validate claims, check quality, retry once if needed, compress context for next round, or degrade gracefully. Rounds 1 and 2 are sequential (R2 depends on R1) and provide the project overview and module boundaries that Rounds 3-6 all consume. Fallbacks ensure the pipeline never crashes -- failed rounds produce raw static data instead.

Output: Four new files: the reusable round execution engine, Round 1 (Project Overview) and Round 2 (Module Detection) step creators, and fallback builders for all 6 rounds.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-ai-analysis-rounds/05-RESEARCH.md
@.planning/phases/05-ai-analysis-rounds/05-01-SUMMARY.md

@src/ai-rounds/types.ts
@src/ai-rounds/schemas.ts
@src/ai-rounds/prompts.ts
@src/ai-rounds/validator.ts
@src/ai-rounds/quality.ts
@src/providers/base.ts
@src/context/compressor.ts
@src/context/tracker.ts
@src/orchestrator/step.ts
@src/domain/types.ts
@src/analyzers/types.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create round execution engine and fallback builders</name>
  <files>src/ai-rounds/runner.ts, src/ai-rounds/fallbacks.ts</files>
  <action>
**Create `src/ai-rounds/runner.ts`:**

Export `executeRound<T>(options)` async function with these parameters bundled in an options object:
- `roundNumber: number`
- `provider: LLMProvider` (from `../providers/base.js`)
- `schema: z.ZodType<T>` (the round's Zod schema)
- `buildPrompt: (isRetry: boolean) => CompletionRequest` (caller provides prompt builder)
- `validate: (data: T) => ValidationResult` (caller provides validation function)
- `buildFallback: () => T` (caller provides fallback builder)
- `tracker: TokenUsageTracker` (from `../context/tracker.js`)
- `estimateTokensFn: (text: string) => number`

Implementation:
1. Track retry state: `let hasRetried = false`
2. Inner `attempt(isRetry: boolean)` async function:
   a. Build prompt via `buildPrompt(isRetry)`
   b. Call `provider.complete(request, schema)` -- this handles Zod validation
   c. Record token usage via `tracker.recordRound()` with round number, input/output tokens, context tokens (estimate from prompt text), file content tokens (0, tracked separately), and budget tokens from `provider.maxContextTokens()`
   d. Validate claims: call `validate(result.data)` to get ValidationResult
   e. If `validation.dropRate > 0.3 && !hasRetried`: set `hasRetried = true`, call `attempt(true)` (locked decision: retry once with stricter prompting)
   f. Quality check: call `checkRoundQuality(result.data as Record<string, unknown>, roundNumber)` from `../ai-rounds/quality.js`
   g. If `!quality.isAcceptable && !hasRetried`: set `hasRetried = true`, call `attempt(true)` (locked decision: retry once)
   h. Compress for next round: call `compressRoundOutput(roundNumber, result.data as Record<string, unknown>, 2000, estimateTokensFn)` from `../context/compressor.js` -- 2000 tokens per prior round (research recommendation)
   i. Return `RoundExecutionResult<T>` with data, validation, quality, context, and status ('retried' if hasRetried, else 'success')
3. Outer try/catch: call `attempt(false)`. On complete failure:
   a. Call `buildFallback()` to get fallback data
   b. Compress fallback data with compressRoundOutput
   c. Return RoundExecutionResult with status 'degraded', zero validation stats, quality isAcceptable=false
   d. Log the error via logger.warn() (do NOT throw -- locked decision: failed rounds degrade, not crash)

**Create `src/ai-rounds/fallbacks.ts`:**

Import StaticAnalysisResult from `../analyzers/types.js`. For each round, export a function that builds a typed fallback from raw static data:

`buildRound1Fallback(analysis: StaticAnalysisResult)`: Return a Round1Output-shaped object:
- projectName from analysis.metadata.rootDir (basename)
- primaryLanguage from analysis.ast.summary.languageBreakdown (most common)
- purpose: '(AI analysis unavailable -- showing static data)'
- technicalLandscape: formatted file stats (total files, total lines, extension breakdown)
- keyDependencies from analysis.dependencies.manifests (map to name/role)
- entryPoints: empty array (can't infer without AI)
- projectScale from analysis.fileTree (file count, estimated complexity by file count thresholds: <50 small, <200 medium, else large)
- techDebt from analysis.todos.items (map TODO/FIXME items to strings)
- findings: ['AI analysis unavailable; showing raw static analysis data']
- openQuestions: []

`buildRound2Fallback(analysis: StaticAnalysisResult)`: Return Round2Output-shaped object:
- modules: top-level directories from directoryTree as module approximations (name from dir name, path from dir path, purpose '(AI analysis unavailable)', publicApi empty, files from children)
- relationships: [] (can't infer without AI)
- boundaryIssues: ['Module boundaries approximated from directory structure -- AI analysis unavailable']
- findings, openQuestions: appropriate placeholders

`buildRound3Fallback`, `buildRound4Fallback`, `buildRound5Fallback`, `buildRound6Fallback`: Similar pattern -- extract whatever static data is relevant to each round's domain, mark as unavailable where AI inference is needed. Keep it simple: the goal is non-empty output so downstream consumers don't crash, not high quality.

For Round 6 specifically: extract env vars from analysis.env, build scripts from analysis.dependencies manifests if available, CI evidence from analysis.fileTree (look for `.github/workflows`, `Dockerfile`, `.gitlab-ci.yml` patterns in directory tree).
  </action>
  <verify>Run `npx tsc --noEmit` to verify compilation. Run `npx tsx -e "import { executeRound } from './src/ai-rounds/runner.js'; import { buildRound1Fallback, buildRound2Fallback } from './src/ai-rounds/fallbacks.js'; console.log(typeof executeRound === 'function' && typeof buildRound1Fallback === 'function' ? 'PASS' : 'FAIL')"` to verify imports.</verify>
  <done>Round execution engine handles the full lifecycle (LLM call, validation, quality check, single retry, compression, fallback). All 6 fallback builders produce typed output from raw static analysis data.</done>
</task>

<task type="auto">
  <name>Task 2: Implement Round 1 (Project Overview) and Round 2 (Module Detection)</name>
  <files>src/ai-rounds/round-1-overview.ts, src/ai-rounds/round-2-modules.ts</files>
  <action>
**Create `src/ai-rounds/round-1-overview.ts`:**

Export `createRound1Step(provider, staticAnalysis, packedContext, config, tracker, estimateTokensFn)` that returns a `StepDefinition`:

```typescript
createStep({
  id: 'ai-round-1',
  name: 'AI Round 1: Project Overview',
  deps: ['static-analysis'],
  execute: async (ctx) => { ... },
  onSkip: () => buildRound1Fallback(staticAnalysis),
})
```

In the execute function:
1. Build round-specific data string from staticAnalysis:
   - File tree summary (total files, dirs, largest files, extension breakdown)
   - Dependency summary (package names and versions from manifests)
   - Git history summary (recent commits, contributors, branch strategy)
   - Existing docs summary (READMEs found, doc coverage %)
   - Business context from `config.context` if present
2. Build prompt: call `buildRoundPrompt(1, ROUND_SYSTEM_PROMPTS[1], packedContext, [], roundData, estimateTokensFn)` -- empty prior rounds since this is Round 1
3. Build validate function: wraps `validateRoundClaims(1, output, staticAnalysis)`
4. Call `executeRound({ roundNumber: 1, provider, schema: Round1OutputSchema, buildPrompt: (isRetry) => ..., validate, buildFallback: () => buildRound1Fallback(staticAnalysis), tracker, estimateTokensFn })`
5. When `isRetry` is true, use `buildRetrySystemPrompt(ROUND_SYSTEM_PROMPTS[1])` for stricter prompting and lower temperature (0.1 per research)
6. Return the RoundExecutionResult

**Create `src/ai-rounds/round-2-modules.ts`:**

Export `createRound2Step(provider, staticAnalysis, packedContext, config, tracker, estimateTokensFn, getRound1Result)` that returns a `StepDefinition`:

The `getRound1Result` parameter is a function `() => RoundExecutionResult<Round1Output> | undefined` that retrieves Round 1's result from the DAG context. This allows Round 2 to access Round 1's compressed context.

```typescript
createStep({
  id: 'ai-round-2',
  name: 'AI Round 2: Module Detection',
  deps: ['ai-round-1'],
  execute: async (ctx) => { ... },
  onSkip: () => buildRound2Fallback(staticAnalysis),
})
```

In the execute function:
1. Get Round 1 result and extract its compressed context
2. Build round-specific data from staticAnalysis:
   - AST summary: import/export counts, language breakdown
   - Directory structure (from fileTree.directoryTree)
   - Import graph: for each file in ast.files, list its imports and what modules import it (build a reverse import map)
   - Export summary: files with most exports (likely module entry points)
3. Build prompt with priorRounds = [round1Result.context] (the compressed Round 1 context)
4. Build validate function: wraps `validateRoundClaims(2, output, staticAnalysis)` -- Round 2 produces file lists and module paths, so validation checks those against known files
5. Call executeRound similarly to Round 1
6. When isRetry, use stricter system prompt with temperature 0.1
7. Return the RoundExecutionResult

Both round files should import from plan 01's modules: schemas, prompts, validator, quality, types. Import createStep from `../orchestrator/step.js`. Import fallbacks from `./fallbacks.js`.

Both rounds must set `maxTokens` on the CompletionRequest. Default 4096 for Round 1, 8192 for Round 2 (module detection may need more output for complex projects, per research open question).
  </action>
  <verify>Run `npx tsc --noEmit` to verify compilation. Run `npx tsx -e "import { createRound1Step } from './src/ai-rounds/round-1-overview.js'; import { createRound2Step } from './src/ai-rounds/round-2-modules.js'; console.log(typeof createRound1Step === 'function' && typeof createRound2Step === 'function' ? 'PASS' : 'FAIL')"` to verify exports.</verify>
  <done>Round 1 produces project overview interleaving business purpose and technical landscape. Round 2 identifies module boundaries from imports, directory structure, and naming patterns. Both integrate with the execution engine for validation, quality checks, single retry, and graceful fallback. Both return StepDefinition for DAG registration.</done>
</task>

</tasks>

<verification>
1. `npx tsc --noEmit` passes with no errors
2. `executeRound` retries at most once (verified by boolean flag logic)
3. Round 1 step has deps=['static-analysis'], Round 2 step has deps=['ai-round-1']
4. Fallback builders produce non-empty output for all 6 rounds
5. Round 1 prompt includes business context from config when available
6. Round 2 prompt includes compressed Round 1 context in prior_analysis section
</verification>

<success_criteria>
- executeRound handles: LLM call -> validate -> quality check -> retry once -> compress -> return
- Round 1 and 2 create StepDefinitions compatible with DAGOrchestrator
- Failed rounds return degraded status with static fallback data (never throw)
- Token usage tracked via TokenUsageTracker for every LLM call
- All locked decisions honored: temperature 0.3 (0.1 on retry), max 1 retry, trust code over model
</success_criteria>

<output>
After completion, create `.planning/phases/05-ai-analysis-rounds/05-02-SUMMARY.md`
</output>
