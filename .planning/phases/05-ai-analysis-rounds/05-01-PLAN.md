---
phase: 05-ai-analysis-rounds
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/ai-rounds/types.ts
  - src/ai-rounds/schemas.ts
  - src/ai-rounds/prompts.ts
  - src/ai-rounds/validator.ts
  - src/ai-rounds/quality.ts
autonomous: true

must_haves:
  truths:
    - "Six round output Zod schemas exist and produce valid JSON Schema via zod-to-json-schema"
    - "Prompt templates for all 6 rounds reference specific codebase data sections with XML tags"
    - "Validation utility can cross-check file path claims and import claims against StaticAnalysisResult"
    - "Quality checker produces isAcceptable boolean based on text length and code reference count"
  artifacts:
    - path: "src/ai-rounds/types.ts"
      provides: "Shared types for round execution, validation, quality, and pipeline summary"
      exports: ["RoundInput", "RoundOutput", "RoundExecutionResult", "ValidationResult", "QualityMetrics", "PipelineValidationSummary", "RoundFallback"]
    - path: "src/ai-rounds/schemas.ts"
      provides: "Zod schemas for all 6 round outputs"
      exports: ["Round1OutputSchema", "Round2OutputSchema", "Round3OutputSchema", "Round4OutputSchema", "Round5ModuleSchema", "Round5OutputSchema", "Round6OutputSchema"]
    - path: "src/ai-rounds/prompts.ts"
      provides: "System prompts and prompt assembly for all 6 rounds"
      exports: ["ROUND_SYSTEM_PROMPTS", "buildRoundPrompt", "buildRetrySystemPrompt"]
    - path: "src/ai-rounds/validator.ts"
      provides: "Hallucination validation against AST-derived facts"
      exports: ["validateFileClaims", "validateImportClaims", "validateRoundClaims"]
    - path: "src/ai-rounds/quality.ts"
      provides: "Quality check heuristics with round-specific thresholds"
      exports: ["checkRoundQuality"]
  key_links:
    - from: "src/ai-rounds/schemas.ts"
      to: "zod"
      via: "z.object() definitions"
      pattern: "z\\.object\\("
    - from: "src/ai-rounds/validator.ts"
      to: "src/analyzers/types.ts"
      via: "StaticAnalysisResult import"
      pattern: "StaticAnalysisResult"
    - from: "src/ai-rounds/prompts.ts"
      to: "src/context/types.ts"
      via: "PackedContext and RoundContext types"
      pattern: "PackedContext|RoundContext"
---

<objective>
Define the foundation layer for all 6 AI analysis rounds: shared types, Zod output schemas, system prompt templates, hallucination validator, and quality checker.

Purpose: Every AI round needs schemas, prompts, validation, and quality checks. Building these as a shared foundation prevents duplication and ensures consistency across rounds. This plan creates no LLM-calling code -- only the definitions and utilities that rounds consume.

Output: Five new files under `src/ai-rounds/` providing the complete type system, schema definitions, prompt templates, validation utilities, and quality check heuristics for Phase 5.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-ai-analysis-rounds/05-RESEARCH.md

@src/domain/types.ts
@src/domain/schemas.ts
@src/analyzers/types.ts
@src/context/types.ts
@src/providers/base.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create shared types and Zod schemas for all 6 rounds</name>
  <files>src/ai-rounds/types.ts, src/ai-rounds/schemas.ts</files>
  <action>
Create `src/ai-rounds/types.ts` with these types (all as TypeScript interfaces, not Zod schemas -- Zod schemas go in schemas.ts):

**RoundInput:**
- `roundNumber: number`
- `staticAnalysis: StaticAnalysisResult` (import from `../analyzers/types.js`)
- `packedContext: PackedContext` (import from `../context/types.js`)
- `priorRounds: RoundContext[]` (import from `../context/types.js`)
- `config: HandoverConfig` (import from `../config/schema.js`)
- `isRetry: boolean`

**RoundExecutionResult<T>:**
- `data: T`
- `validation: ValidationResult`
- `quality: QualityMetrics`
- `context: RoundContext` (compressed context for next round)
- `status: 'success' | 'degraded' | 'retried'`

**ValidationResult:**
- `validated: number` (claims that passed)
- `corrected: number` (claims dropped silently)
- `total: number` (total validatable claims)
- `dropRate: number` (corrected / total)

**QualityMetrics:**
- `textLength: number`
- `codeReferences: number`
- `specificity: number` (ratio of code refs per 100 chars)
- `isAcceptable: boolean`

**RoundFallback:**
- `roundNumber: number`
- `status: 'degraded' | 'failed'`
- `reason: string`
- `staticFallback: Record<string, unknown>`

**PipelineValidationSummary:**
- `totalClaims: number`
- `validatedClaims: number`
- `correctedClaims: number`
- `roundSummaries: Array<{ round: number; name: string; status: 'success' | 'degraded' | 'retried' | 'skipped' | 'failed'; validated: number; corrected: number; reason?: string }>`

**ROUND_NAMES constant:** `Record<number, string>` mapping 1-6 to round names (e.g., 1: 'Project Overview', 2: 'Module Detection', etc.)

Create `src/ai-rounds/schemas.ts` with Zod schemas for all 6 round outputs:

**Round1OutputSchema** (AI-01: Project Overview):
- `projectName: z.string()`
- `primaryLanguage: z.string()`
- `framework: z.string().optional()`
- `purpose: z.string()` -- business purpose
- `technicalLandscape: z.string()` -- technical overview
- `keyDependencies: z.array(z.object({ name: z.string(), role: z.string() }))`
- `entryPoints: z.array(z.object({ path: z.string(), type: z.string(), description: z.string() }))`
- `projectScale: z.object({ fileCount: z.number(), estimatedComplexity: z.enum(['small', 'medium', 'large']), mainConcerns: z.array(z.string()) })`
- `techDebt: z.array(z.string())` -- directly observed issues
- `findings: z.array(z.string())`
- `openQuestions: z.array(z.string())`

**Round2OutputSchema** (AI-02: Module Detection):
- `modules: z.array(z.object({ name: z.string(), path: z.string(), purpose: z.string(), publicApi: z.array(z.string()), files: z.array(z.string()), concerns: z.array(z.string()).optional() }))`
- `relationships: z.array(z.object({ from: z.string(), to: z.string(), type: z.string(), evidence: z.string() }))`
- `boundaryIssues: z.array(z.string())` -- mixed concerns, unclear boundaries
- `findings: z.array(z.string())`
- `openQuestions: z.array(z.string())`

**Round3OutputSchema** (AI-03: Feature Extraction):
- `features: z.array(z.object({ name: z.string(), description: z.string(), modules: z.array(z.string()), entryPoint: z.string(), files: z.array(z.string()), userFacing: z.boolean() }))`
- `crossModuleFlows: z.array(z.object({ name: z.string(), path: z.array(z.string()), description: z.string() }))` -- per user decision: trace features across modules even when uncertain
- `findings: z.array(z.string())`

**Round4OutputSchema** (AI-04: Architecture Detection):
- `patterns: z.array(z.object({ name: z.string(), confidence: z.enum(['high']), evidence: z.array(z.string()), modules: z.array(z.string()), description: z.string() }))` -- per user decision: only high confidence patterns
- `layering: z.object({ layers: z.array(z.object({ name: z.string(), modules: z.array(z.string()), responsibility: z.string() })) }).optional()`
- `dataFlow: z.array(z.object({ from: z.string(), to: z.string(), data: z.string(), mechanism: z.string() }))`
- `findings: z.array(z.string())`

**Round5ModuleSchema** (AI-05: Per-module edge cases):
- `moduleName: z.string()`
- `edgeCases: z.array(z.object({ description: z.string(), file: z.string(), line: z.number().optional(), severity: z.enum(['critical', 'warning', 'info']), evidence: z.string() }))` -- per user decision: only provable issues
- `conventions: z.array(z.object({ pattern: z.string(), examples: z.array(z.string()), description: z.string() }))`
- `errorHandling: z.object({ strategy: z.string(), gaps: z.array(z.string()), patterns: z.array(z.string()) })`
- `findings: z.array(z.string())`

**Round5OutputSchema** (aggregated):
- `modules: z.array(Round5ModuleSchema)`
- `crossCuttingConventions: z.array(z.object({ pattern: z.string(), description: z.string(), frequency: z.string() }))`
- `findings: z.array(z.string())`

**Round6OutputSchema** (AI-06: Deployment Inference):
- `deployment: z.object({ platform: z.string().optional(), containerized: z.boolean(), ciProvider: z.string().optional(), evidence: z.array(z.string()) })`
- `envVars: z.array(z.object({ name: z.string(), purpose: z.string(), required: z.boolean(), source: z.string() }))`
- `buildProcess: z.object({ commands: z.array(z.string()), artifacts: z.array(z.string()), scripts: z.record(z.string(), z.string()) })`
- `infrastructure: z.array(z.object({ service: z.string(), purpose: z.string(), evidence: z.string() }))`
- `findings: z.array(z.string())`

Keep schemas flat (no deeply nested `$ref` structures). Use `z.string()` for free-form analysis text. Put structured data in arrays of simple objects. This avoids Pitfall 2 from research (Zod schema too complex for tool_use).

Export type aliases via `z.infer<typeof Schema>` for each schema (e.g., `export type Round1Output = z.infer<typeof Round1OutputSchema>`).
  </action>
  <verify>Run `npx tsx -e "import { Round1OutputSchema, Round2OutputSchema, Round3OutputSchema, Round4OutputSchema, Round5OutputSchema, Round6OutputSchema } from './src/ai-rounds/schemas.js'; import { zodToJsonSchema } from 'zod-to-json-schema'; console.log(Object.keys(zodToJsonSchema(Round1OutputSchema, 'r')).length > 0 ? 'PASS' : 'FAIL')"` -- schemas import cleanly and convert to JSON Schema</verify>
  <done>All 6 round Zod schemas export and convert to JSON Schema. Shared types file exports all interfaces. Type aliases derived from schemas.</done>
</task>

<task type="auto">
  <name>Task 2: Create prompt templates, validator, and quality checker</name>
  <files>src/ai-rounds/prompts.ts, src/ai-rounds/validator.ts, src/ai-rounds/quality.ts</files>
  <action>
**Create `src/ai-rounds/prompts.ts`:**

Export `ROUND_SYSTEM_PROMPTS: Record<number, string>` with system prompts for each round:

Round 1 system prompt (from research): Senior software architect analyzing for handover documentation. Interleave business purpose with technical landscape (locked decision). Direct and honest about tech debt. Layered output: senior-engineer overview first, depth for juniors after. Must reference specific files. XML tag structure.

Round 2 system prompt (from research): Identify module boundaries, even when code lacks explicit separation (locked decision). Infer from imports, directory structure, naming, cohesion. Each module: name, path, purpose, public API. Be honest about mixed concerns.

Round 3 system prompt: Trace user-facing features across modules. Cross-module tracing even when uncertain (locked decision). Map features to their entry points and file paths.

Round 4 system prompt: Identify architecture patterns (MVC, event-driven, CQRS, etc.). Only state patterns with high confidence (locked decision). No hedging or uncertain matches. Describe data flow between layers.

Round 5 system prompt: Per-module analysis of edge cases, conventions, and error handling. Only flag provable issues evidenced in code (locked decision). No speculative "potential race condition" flags. Cite file and line for every edge case.

Round 6 system prompt: Deployment inference -- best effort always (locked decision). Piece together signals from Dockerfile, env vars, scripts, CI configs. Document build process, infrastructure dependencies, env var requirements.

Export `buildRoundPrompt(roundNumber, systemInstructions, packedContext, priorRounds, roundSpecificData, estimateTokensFn): CompletionRequest`:
- Assemble user prompt with XML-tagged sections: `<codebase_context>`, `<prior_analysis>`, `<round_data>`, `<instructions>`
- Prior context from RoundContext[] formatted as "Round N Context: Modules, Findings, Relationships"
- File content from PackedContext files (filter out `tier === 'skip'`, format as `### path\n```\ncontent\n```)
- Set `temperature: 0.3` for all rounds (locked research recommendation)
- Set `maxTokens: 4096` default, but accept optional override
- Import CompletionRequest type from `../domain/types.js`

Export `buildRetrySystemPrompt(basePrompt: string): string`:
- Prepend: "IMPORTANT: Your previous attempt was too generic. You MUST reference specific files, functions, and code patterns from the provided codebase. Every claim must cite a file path."
- Append: "If you are uncertain about a claim, omit it entirely rather than stating it vaguely."

**Create `src/ai-rounds/validator.ts`:**

Export `validateFileClaims(claimedPaths: string[], analysis: StaticAnalysisResult): { valid: string[]; dropped: string[] }`:
- Build a Set of known file paths from `analysis.fileTree.directoryTree` (entries where `type === 'file'`)
- For each claimed path: if in Set, valid; otherwise dropped silently (locked decision: trust code over model)

Export `validateImportClaims(claims: Array<{ from: string; to: string }>, analysis: StaticAnalysisResult): { valid: typeof claims; dropped: typeof claims }`:
- Build a Map from `analysis.ast.files`: each file path -> Set of import sources
- For each claim: check if `from` file exists AND imports `to`. If not, drop silently.

Export `validateRoundClaims(roundNumber: number, output: Record<string, unknown>, analysis: StaticAnalysisResult): ValidationResult`:
- Extract file path claims from output (scan all string values matching file-path-like patterns: contains `/` or `\` and ends with common extensions, or matches known project paths)
- Extract import claims where applicable (Round 2 relationships, Round 3 cross-module flows)
- Call validateFileClaims and validateImportClaims
- Return ValidationResult with counts
- Scope: critical claims only (locked decision) -- file references and import/dependency claims. Do NOT validate high-level observations.

**Create `src/ai-rounds/quality.ts`:**

Export `checkRoundQuality(output: Record<string, unknown>, roundNumber: number): QualityMetrics`:
- Serialize output to JSON string for analysis
- Count code references: regex pattern `/(?:src\/|\.ts|\.js|\.py|\.rs|\.go|function\s+\w+|class\s+\w+)/g`
- Thresholds (from research):
  - Min text length: 500 chars for Rounds 1-5, 200 chars for Round 6
  - Min code references: 3 for Round 1, 5 for Rounds 2-5, 2 for Round 6
  - Zero file path references always fails regardless of length
- Calculate specificity: codeReferences / max(textLength / 100, 1)
- Return QualityMetrics with isAcceptable based on thresholds
  </action>
  <verify>Run `npx tsc --noEmit` to verify all files compile without errors. Run `npx tsx -e "import { validateFileClaims } from './src/ai-rounds/validator.js'; import { checkRoundQuality } from './src/ai-rounds/quality.js'; import { ROUND_SYSTEM_PROMPTS } from './src/ai-rounds/prompts.js'; console.log(Object.keys(ROUND_SYSTEM_PROMPTS).length === 6 ? 'PASS' : 'FAIL')"` to verify imports work.</verify>
  <done>Prompt templates cover all 6 rounds with user-decision-aligned instructions. Validator checks file paths and import claims against AST data. Quality checker produces threshold-based isAcceptable flag. All three files compile and import cleanly.</done>
</task>

</tasks>

<verification>
1. `npx tsc --noEmit` passes with no errors in `src/ai-rounds/` files
2. All 6 Zod schemas convert to JSON Schema via `zodToJsonSchema()` without errors
3. Validator can be called with a mock StaticAnalysisResult and produces valid/dropped arrays
4. Quality checker returns isAcceptable=false for empty output and isAcceptable=true for content with code references
5. Prompt builder produces a CompletionRequest with systemPrompt, userPrompt, temperature=0.3
</verification>

<success_criteria>
- All 5 files exist under `src/ai-rounds/`
- Zod schemas for 6 rounds are flat (no deeply nested $ref) and export type aliases
- System prompts honor all locked decisions (tone, confidence handling, validation scope)
- Validator cross-checks file paths and import claims only (not high-level observations)
- Quality thresholds match research recommendations (500/200 chars, 3-5/2 code refs)
- TypeScript compilation succeeds with no errors
</success_criteria>

<output>
After completion, create `.planning/phases/05-ai-analysis-rounds/05-01-SUMMARY.md`
</output>
